{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aycKtLe-kUbN"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Tennis Player Tracking and Kinematic Analysis Pipeline\n",
        "#\n",
        "# Author: Peterson Antonio\n",
        "# Affiliation: FCMax Performance\n",
        "#\n",
        "# Description:\n",
        "# This notebook implements a complete computer vision pipeline for\n",
        "# tennis player tracking and motion analysis using YOLOv12, ByteTrack,\n",
        "# court keypoint detection, homography transformation, and temporal filtering.\n",
        "#\n",
        "# The pipeline enables conversion from image-space coordinates (pixels)\n",
        "# to real-world coordinates (meters), allowing quantitative analysis of:\n",
        "#\n",
        "# • Player trajectories\n",
        "# • Distance covered\n",
        "# • Velocity\n",
        "# • Acceleration and deceleration\n",
        "#\n",
        "# Core components:\n",
        "# • YOLOv12 for object detection\n",
        "# • ByteTrack for multi-object tracking\n",
        "# • Court keypoint detection for homography estimation\n",
        "# • Ground-plane coordinate transformation\n",
        "# • Temporal filtering and interpolation\n",
        "# • Kinematic metrics computation\n",
        "#\n",
        "# Applications:\n",
        "# • Sports performance analysis\n",
        "# • Player movement quantification\n",
        "# • Computer vision research\n",
        "# • Automated sports analytics systems\n",
        "#\n",
        "# ============================================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f77XLLwROJaE"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# ENVIRONMENT INITIALIZATION — Environment setup and project loading\n",
        "#\n",
        "# This block initializes the Google Colab runtime environment and prepares\n",
        "# the workspace for execution of the analysis pipeline.\n",
        "#\n",
        "# Main objectives:\n",
        "#\n",
        "# 1. Mount Google Drive\n",
        "#    - Provides access to project files, including:\n",
        "#        • input videos\n",
        "#        • trained models\n",
        "#        • configuration files\n",
        "#        • output directories\n",
        "#\n",
        "# 2. Define the project working directory\n",
        "#    - Ensures all relative paths function correctly\n",
        "#    - Centralizes execution within the main project folder\n",
        "#\n",
        "# 3. Install dependencies\n",
        "#    - Automatically installs all required libraries as specified\n",
        "#      in the requirements.txt file\n",
        "#\n",
        "# Benefits:\n",
        "# - Ensures environment reproducibility\n",
        "# - Prevents import errors and version incompatibilities\n",
        "# - Allows consistent execution across different Colab sessions\n",
        "#\n",
        "# Technical note:\n",
        "# This block should be executed once at the beginning of the session.\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "# === Standard initialization block ===\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# 1. Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Define project path (adjust if located elsewhere in your Drive)\n",
        "project_path = \"/content/drive/MyDrive/tennis_analysis\"\n",
        "\n",
        "# 3. Change to project directory\n",
        "os.chdir(project_path)\n",
        "print(\"Current working directory:\", os.getcwd())\n",
        "\n",
        "# 4. Install dependencies from requirements.txt\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPgPe705zUNG"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# TRACKER CONFIGURATION — Mode 1: Fixed environment-specific path\n",
        "#\n",
        "# This mode loads the default ByteTrack configuration file using an absolute\n",
        "# filesystem path and creates a modified version with custom tracking parameters.\n",
        "#\n",
        "# Characteristics:\n",
        "#\n",
        "# • Simple and direct implementation\n",
        "# • Uses a fixed path to the installed Ultralytics tracker configuration\n",
        "# • Environment-dependent (Python version, OS, and installation location)\n",
        "#\n",
        "# Limitation:\n",
        "#\n",
        "# This approach may require manual path adjustment when executed on a different\n",
        "# system, virtual environment, or Python installation.\n",
        "#\n",
        "# Recommended usage:\n",
        "#\n",
        "# Suitable for controlled environments such as Google Colab or a fixed local setup.\n",
        "#\n",
        "# Output:\n",
        "#\n",
        "# Generates a modified ByteTrack configuration file for improved tracking stability.\n",
        "#\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "# Generate a modified ByteTrack configuration file\n",
        "import yaml\n",
        "\n",
        "# Path to the default tracker configuration file\n",
        "tracker_path = \"/usr/local/lib/python3.12/dist-packages/ultralytics/cfg/trackers/bytetrack.yaml\"\n",
        "\n",
        "# Load configuration\n",
        "with open(tracker_path, \"r\") as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# Modify tracking parameters\n",
        "config[\"track_buffer\"] = 100        # increases track persistence during temporary occlusions\n",
        "config[\"new_track_thresh\"] = 0.7    # confidence threshold for initializing new tracks\n",
        "\n",
        "# Save modified configuration file\n",
        "with open(\"/content/drive/MyDrive/tennis_analysis/bytetrack_modified.yaml\", \"w\") as f:\n",
        "    yaml.dump(config, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpIEhZQ3QtgG"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# TRACKER CONFIGURATION — Mode 2: Dynamic environment-independent path (recommended)\n",
        "#\n",
        "# This mode automatically detects the installation location of the Ultralytics\n",
        "# package and accesses the correct ByteTrack configuration file.\n",
        "#\n",
        "# Characteristics:\n",
        "#\n",
        "# • Fully portable across environments\n",
        "# • Works on Google Colab, Windows, Linux, virtual environments, and conda\n",
        "# • Independent of Python version or installation path\n",
        "# • Does not require manual path adjustments\n",
        "#\n",
        "# Recommended usage:\n",
        "#\n",
        "# This is the preferred approach for shared notebooks, GitHub repositories,\n",
        "# and production pipelines where portability and reproducibility are required.\n",
        "#\n",
        "# Output:\n",
        "#\n",
        "# Generates a modified ByteTrack configuration file with customized tracking\n",
        "# parameters, saved in the current project directory.\n",
        "#\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "# Generate a modified ByteTrack configuration file (portable mode)\n",
        "\n",
        "import yaml\n",
        "import ultralytics\n",
        "import os\n",
        "\n",
        "# Automatically detect Ultralytics installation path\n",
        "ultra_path = os.path.dirname(ultralytics.__file__)\n",
        "\n",
        "# Build path to default ByteTrack configuration file\n",
        "tracker_path = os.path.join(\n",
        "    ultra_path,\n",
        "    \"cfg\",\n",
        "    \"trackers\",\n",
        "    \"bytetrack.yaml\"\n",
        ")\n",
        "\n",
        "print(\"Using original tracker configuration from:\")\n",
        "print(tracker_path)\n",
        "\n",
        "# Load original configuration\n",
        "with open(tracker_path, \"r\") as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# Modify tracking parameters\n",
        "config[\"track_buffer\"] = 100        # increases track persistence during temporary occlusions\n",
        "config[\"new_track_thresh\"] = 0.7    # confidence threshold for initializing new tracks\n",
        "\n",
        "# Define portable output path (current project directory)\n",
        "output_path = os.path.join(os.getcwd(), \"bytetrack_modified.yaml\")\n",
        "\n",
        "# Save modified configuration\n",
        "with open(output_path, \"w\") as f:\n",
        "    yaml.dump(config, f, sort_keys=False)\n",
        "\n",
        "print(\"\\nModified configuration file saved at:\")\n",
        "print(output_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IcU_lpLj4fE"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# OBJECT DETECTION AND TRACKING — Player detection and tracking with YOLOv12 + ByteTrack\n",
        "#\n",
        "# This module performs multi-object player detection and tracking using YOLOv12\n",
        "# integrated with the ByteTrack algorithm for temporal ID association.\n",
        "#\n",
        "# Main goals:\n",
        "# - Detect players in each video frame\n",
        "# - Assign persistent IDs over time (multi-object tracking)\n",
        "# - Extract stable coordinates for downstream kinematic analysis\n",
        "#\n",
        "# Pipeline overview:\n",
        "#\n",
        "# 1. YOLOv12 initialization\n",
        "#    - Deep learning-based object detector optimized for real-time inference\n",
        "#      with a strong accuracy/efficiency trade-off.\n",
        "#\n",
        "# 2. Multi-object tracking with ByteTrack\n",
        "#    - Associates detections across consecutive frames\n",
        "#    - Maintains persistent IDs for each player\n",
        "#    - Uses a customized tracker configuration (bytetrack_modified.yaml) to\n",
        "#      improve temporal stability and reduce ID switches.\n",
        "#\n",
        "#    Key parameters:\n",
        "#      persist=True  → keeps tracker state across frames\n",
        "#      tracker=...   → path to custom ByteTrack configuration\n",
        "#\n",
        "# 3. Player position extraction (footpoint heuristic)\n",
        "#    - For each detected bounding box:\n",
        "#\n",
        "#        cx = horizontal center of the bounding box\n",
        "#        cy = bottom edge of the bounding box (y2)\n",
        "#\n",
        "#    - The point (cx, cy) approximates the player's ground contact position\n",
        "#      (ground-plane proxy), which is suitable for homography-based mapping\n",
        "#      into real-world court coordinates.\n",
        "#\n",
        "# 4. Tabular data structure (pandas DataFrame)\n",
        "#\n",
        "#    Output format:\n",
        "#\n",
        "#        id1_x | id1_y | id2_x | id2_y | ...\n",
        "#\n",
        "#    Each row corresponds to a video frame.\n",
        "#    Each column pair corresponds to a tracked player ID.\n",
        "#    Missing detections are stored as NaN.\n",
        "#\n",
        "# Downstream usage:\n",
        "# - Primary player selection (2-player extraction)\n",
        "# - Homography transformation (pixels → meters)\n",
        "# - Kinematic metrics computation (distance, speed, acceleration)\n",
        "# - Trajectory analysis and visualization\n",
        "#\n",
        "# Technical note:\n",
        "# The footpoint heuristic is a standard approach in sports tracking pipelines\n",
        "# when direct 3D pose/foot contact estimation is not available.\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "# ------------------------------- IMPORTANT --------------------------------\n",
        "# Upload your input video to the `input_videos/` folder and update the path below.\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "%cd /content/drive/MyDrive/tennis_analysis\n",
        "\n",
        "from ultralytics import YOLO\n",
        "import pandas as pd\n",
        "\n",
        "# Load the YOLOv12 model\n",
        "model = YOLO(\"yolo12n.pt\")\n",
        "\n",
        "# Input video path\n",
        "source = \"/content/drive/MyDrive/tennis_analysis/input_videos/Djokovic.mp4\"\n",
        "\n",
        "# Run tracking (YOLOv12 + ByteTrack)\n",
        "# NOTE: `tracker` points to a customized ByteTrack config file.\n",
        "results = model.track(\n",
        "    source,\n",
        "    save=True,\n",
        "    persist=True,\n",
        "    tracker=\"/content/drive/MyDrive/tennis_analysis/bytetrack_modified.yaml\"\n",
        ")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Convert tracking output into a per-frame DataFrame\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "frames_data = []\n",
        "\n",
        "for res in results:\n",
        "    frame_dict = {}\n",
        "\n",
        "    if res.boxes.id is not None:  # check if the frame contains tracked IDs\n",
        "        ids = res.boxes.id.cpu().numpy().astype(int)\n",
        "        xyxy = res.boxes.xyxy.cpu().numpy()  # [x1, y1, x2, y2]\n",
        "\n",
        "        for obj_id, box in zip(ids, xyxy):\n",
        "            cx = (box[0] + box[2]) / 2.0  # bbox center-x\n",
        "            cy = box[3]                   # bbox bottom (y2) → footpoint heuristic\n",
        "            frame_dict[f\"id{obj_id}_x\"] = cx\n",
        "            frame_dict[f\"id{obj_id}_y\"] = cy\n",
        "\n",
        "    frames_data.append(frame_dict)\n",
        "\n",
        "# Build DataFrame (NaNs for frames where an ID was not detected)\n",
        "player_detections = pd.DataFrame(frames_data)\n",
        "\n",
        "# Sort columns by ID index, keeping x/y pairs grouped\n",
        "player_detections = player_detections.reindex(\n",
        "    sorted(\n",
        "        player_detections.columns,\n",
        "        key=lambda x: (int(x.split('_')[0][2:]), x[-1])\n",
        "    ),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "print(player_detections.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08VvVXA0kIFB"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# COURT KEYPOINT DETECTION — Extraction of court geometric reference points\n",
        "#\n",
        "# This module automatically detects structural keypoints of the tennis court\n",
        "# using a deep learning model trained to identify line intersections and\n",
        "# court boundaries.\n",
        "#\n",
        "# Main objective:\n",
        "# Provide accurate correspondences between image-space coordinates (pixels)\n",
        "# and real-world court coordinates (meters), enabling homography computation.\n",
        "#\n",
        "# Pipeline:\n",
        "#\n",
        "# 1. Video loading\n",
        "#    - The video is loaded into memory as a sequence of frames.\n",
        "#\n",
        "# 2. Court keypoint detection model initialization\n",
        "#    - CNN-based model trained specifically to detect structural reference\n",
        "#      points of a tennis court.\n",
        "#\n",
        "# 3. Keypoint detection on the first frame\n",
        "#    - Assumes a static camera setup, allowing the use of a single fixed\n",
        "#      homography for the entire video sequence.\n",
        "#    - Returns keypoint coordinates in image-space.\n",
        "#\n",
        "# Output:\n",
        "# court_keypoints → numpy array of shape (N, 2)\n",
        "# containing detected court keypoints in pixel coordinates (x, y).\n",
        "#\n",
        "# Downstream usage:\n",
        "# These keypoints are used to compute the homography matrix that maps player\n",
        "# positions from image-space (pixels) to real-world court coordinates (meters).\n",
        "#\n",
        "# Assumption:\n",
        "# The camera remains fixed throughout the video. If the camera moves,\n",
        "# homography must be recomputed dynamically per frame.\n",
        "# ============================================================\n",
        "\n",
        "from utils import read_video\n",
        "from court_line_detector import CourtLineDetector\n",
        "\n",
        "# Load video frames into memory\n",
        "source = \"/content/drive/MyDrive/tennis_analysis/input_videos/Djokovic.mp4\"\n",
        "video_frames = read_video(source)\n",
        "\n",
        "# Initialize court keypoint detection model\n",
        "court_model_path = \"/content/drive/MyDrive/tennis_analysis/models/keypoints_model_50.pth\"\n",
        "court_line_detector = CourtLineDetector(court_model_path)\n",
        "\n",
        "# Detect court keypoints from the first frame\n",
        "court_keypoints = court_line_detector.predict(video_frames[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUE4jbHZkmOT"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# PLAYER VISUALIZATION PIPELINE — Selection, point rendering, and temporal motion trail\n",
        "#\n",
        "# This module automatically identifies the two primary tracked players and\n",
        "# generates a visualization video with a simplified kinematic representation.\n",
        "#\n",
        "# Key features:\n",
        "#\n",
        "# 1. Automatic selection of the two main players\n",
        "#    - Scores all tracked IDs based on:\n",
        "#        • number of valid frames (track continuity)\n",
        "#        • spatial consistency relative to the court center\n",
        "#    - Selects the two most stable and relevant IDs for analysis.\n",
        "#\n",
        "# 2. Point-based representation using the footpoint heuristic\n",
        "#    - Each player is represented by the bottom-center of the bounding box (cx, y2),\n",
        "#      which approximates ground contact position (ground-plane proxy).\n",
        "#    - This reduces visual clutter while keeping a consistent proxy of player position\n",
        "#      on the court plane.\n",
        "#\n",
        "# 3. Optional temporal trail rendering (\"motion brush\")\n",
        "#    - Accumulates player positions over time to visualize trajectories and\n",
        "#      movement patterns clearly.\n",
        "#    - The `trail_decay` parameter controls temporal fade-out:\n",
        "#        • 1.0  → infinite trail (no decay)\n",
        "#        • <1.0 → trail gradually fades over time\n",
        "#\n",
        "# 4. Court keypoint overlay\n",
        "#    - Renders detected court keypoints to provide geometric context.\n",
        "#\n",
        "# 5. Optional ball rendering support\n",
        "#    - If ball detections are provided, the ball is rendered as a point using\n",
        "#      the same bottom-center heuristic.\n",
        "#\n",
        "# Use cases:\n",
        "# - Visual QA of tracking stability (ID switches, drift, missed detections)\n",
        "# - Qualitative trajectory inspection prior to quantitative analysis\n",
        "# - Debugging and demonstration video generation\n",
        "#\n",
        "# Technical note:\n",
        "# This module is for visualization only; it does not modify the coordinate data\n",
        "# used for homography estimation or metric computation.\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "def normalize_court_keypoints(court_keypoints):\n",
        "    arr = np.array(court_keypoints, dtype=float)\n",
        "\n",
        "    if arr.ndim == 0:\n",
        "        raise ValueError(\"court_keypoints is scalar; provide a list/array of points (N, 2).\")\n",
        "\n",
        "    if arr.ndim == 1:\n",
        "        if arr.size % 2 == 0:\n",
        "            arr = arr.reshape(-1, 2)\n",
        "        else:\n",
        "            raise ValueError(\"1D array with odd length cannot be interpreted as (x, y) pairs.\")\n",
        "\n",
        "    elif arr.ndim == 2:\n",
        "        if arr.shape[1] != 2:\n",
        "            raise ValueError(f\"court_keypoints with shape {arr.shape} is not in (N, 2) format.\")\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"court_keypoints with ndim={arr.ndim} is not supported.\")\n",
        "\n",
        "    return arr\n",
        "\n",
        "\n",
        "def select_two_players_balanced(player_detections: pd.DataFrame, court_keypoints, alpha=0.5):\n",
        "    kp = normalize_court_keypoints(court_keypoints)\n",
        "    court_center = np.mean(kp, axis=0)\n",
        "\n",
        "    metrics = {}\n",
        "    x_cols = [c for c in player_detections.columns if c.endswith(\"_x\")]\n",
        "\n",
        "    for col_x in x_cols:\n",
        "        player_id = col_x.rsplit(\"_\", 1)[0]\n",
        "        col_y = f\"{player_id}_y\"\n",
        "        if col_y not in player_detections.columns:\n",
        "            continue\n",
        "\n",
        "        x_vals = pd.to_numeric(player_detections[col_x], errors=\"coerce\").to_numpy(dtype=float)\n",
        "        y_vals = pd.to_numeric(player_detections[col_y], errors=\"coerce\").to_numpy(dtype=float)\n",
        "        valid_mask = np.isfinite(x_vals) & np.isfinite(y_vals)\n",
        "\n",
        "        n_valid = int(np.sum(valid_mask))\n",
        "        if n_valid == 0 or n_valid < len(player_detections[col_x]) * 0.5:\n",
        "            continue\n",
        "\n",
        "        dx = x_vals[valid_mask] - court_center[0]\n",
        "        dy = y_vals[valid_mask] - court_center[1]\n",
        "        dist_mean = float(np.mean(np.sqrt(dx * dx + dy * dy)))\n",
        "\n",
        "        # Lower score is better: closer to court center + more valid frames\n",
        "        score = dist_mean / (n_valid ** alpha)\n",
        "        metrics[player_id] = {\"score\": score, \"dist_mean\": dist_mean, \"n_valid\": n_valid}\n",
        "\n",
        "    if len(metrics) < 2:\n",
        "        raise ValueError(\"Fewer than two tracked IDs with sufficient valid data were found.\")\n",
        "\n",
        "    sorted_ids = sorted(metrics.keys(), key=lambda k: metrics[k][\"score\"])[:2]\n",
        "\n",
        "    selected_cols = []\n",
        "    for pid in sorted_ids:\n",
        "        selected_cols.extend([f\"{pid}_x\", f\"{pid}_y\"])\n",
        "\n",
        "    players_detections_selected = player_detections.reindex(columns=selected_cols).copy()\n",
        "    players_detections_selected.columns = [\"player1_x\", \"player1_y\", \"player2_x\", \"player2_y\"]\n",
        "\n",
        "    id_map = {\"player1\": sorted_ids[0], \"player2\": sorted_ids[1]}\n",
        "    return players_detections_selected, sorted_ids, id_map\n",
        "\n",
        "\n",
        "def draw_selected_players_and_court(\n",
        "    video_frames,\n",
        "    results,\n",
        "    selected_ids,\n",
        "    court_keypoints,\n",
        "    output_path=\"players_and_court.mp4\",\n",
        "    fps=30,\n",
        "    # optional: ball detections\n",
        "    ball_detections=None,\n",
        "    ball_color=(0, 0, 255),\n",
        "    ball_radius=5,\n",
        "    # point + trail\n",
        "    point_radius=6,\n",
        "    draw_trail=True,\n",
        "    trail_decay=0.96,   # 1.0 = infinite trail; <1.0 = trail fades over time\n",
        "    trail_mix=0.75,     # 0 = trail only, 1 = frame only\n",
        "):\n",
        "    \"\"\"\n",
        "    Generates a visualization video with:\n",
        "      - court keypoints overlay\n",
        "      - two players rendered as POINTS (no bounding boxes), using the bbox bottom-center (cx, y2)\n",
        "      - optional accumulated motion trail (\"brush\")\n",
        "      - optional ball rendering as a point (bbox bottom-center)\n",
        "\n",
        "    Parameters:\n",
        "      - point_radius: player marker radius in pixels\n",
        "      - draw_trail: if True, renders the accumulated motion trail\n",
        "      - trail_decay: trail decay factor (0.90–0.99 typical). Use 1.0 for no decay.\n",
        "      - trail_mix: blend ratio between current frame and trail (0.6–0.85 typical)\n",
        "    \"\"\"\n",
        "    dynamic_kp = isinstance(court_keypoints, (list, tuple)) and len(court_keypoints) == len(video_frames)\n",
        "    kp_static = None if dynamic_kp else normalize_court_keypoints(court_keypoints)\n",
        "\n",
        "    colors = [(0, 255, 0), (255, 0, 0)]  # player1 green, player2 blue\n",
        "\n",
        "    h, w = video_frames[0].shape[:2]\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (w, h))\n",
        "\n",
        "    # persistent trail canvas\n",
        "    trail = np.zeros_like(video_frames[0], dtype=np.uint8) if draw_trail else None\n",
        "\n",
        "    def _extract_boxes(det_obj):\n",
        "        boxes = []\n",
        "        if det_obj is None:\n",
        "            return boxes\n",
        "\n",
        "        if isinstance(det_obj, (list, tuple, np.ndarray)):\n",
        "            arr = np.array(det_obj)\n",
        "            if arr.ndim == 2 and arr.shape[1] >= 4:\n",
        "                for row in arr:\n",
        "                    boxes.append([float(row[0]), float(row[1]), float(row[2]), float(row[3])])\n",
        "                return boxes\n",
        "            for item in det_obj:\n",
        "                try:\n",
        "                    a = np.array(item)\n",
        "                    if a.size >= 4:\n",
        "                        boxes.append([float(a.flatten()[0]), float(a.flatten()[1]),\n",
        "                                      float(a.flatten()[2]), float(a.flatten()[3])])\n",
        "                except Exception:\n",
        "                    continue\n",
        "            return boxes\n",
        "\n",
        "        if hasattr(det_obj, \"boxes\"):\n",
        "            xyxy = getattr(det_obj.boxes, \"xyxy\", None)\n",
        "            if xyxy is not None:\n",
        "                try:\n",
        "                    arr = xyxy.cpu().numpy() if hasattr(xyxy, \"cpu\") else np.array(xyxy)\n",
        "                    for row in arr:\n",
        "                        boxes.append([float(row[0]), float(row[1]), float(row[2]), float(row[3])])\n",
        "                    return boxes\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "        try:\n",
        "            arr = np.array(det_obj, dtype=float)\n",
        "            if arr.ndim == 1 and arr.size % 4 == 0 and arr.size >= 4:\n",
        "                arr2 = arr.reshape(-1, 4)\n",
        "                for row in arr2:\n",
        "                    boxes.append([float(row[0]), float(row[1]), float(row[2]), float(row[3])])\n",
        "                return boxes\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        return boxes\n",
        "\n",
        "    n_frames = min(len(video_frames), len(results))\n",
        "    if ball_detections is not None:\n",
        "        n_frames = min(n_frames, len(ball_detections))\n",
        "\n",
        "    for idx in range(n_frames):\n",
        "        frame = video_frames[idx]\n",
        "        res = results[idx]\n",
        "        img = frame.copy()\n",
        "\n",
        "        kp_frame = normalize_court_keypoints(court_keypoints[idx]) if dynamic_kp else kp_static\n",
        "\n",
        "        # draw court keypoints\n",
        "        for (x, y) in kp_frame:\n",
        "            cv2.circle(img, (int(round(x)), int(round(y))), 4, (0, 255, 255), -1)\n",
        "\n",
        "        # decay trail\n",
        "        if draw_trail and trail is not None and trail_decay < 1.0:\n",
        "            trail[:] = (trail.astype(np.float32) * float(trail_decay)).astype(np.uint8)\n",
        "\n",
        "        # draw players as points (bbox bottom-center)\n",
        "        try:\n",
        "            if hasattr(res, \"boxes\") and res.boxes is not None and getattr(res.boxes, \"id\", None) is not None:\n",
        "                ids = res.boxes.id.cpu().numpy().astype(int)\n",
        "                xyxy = res.boxes.xyxy.cpu().numpy()\n",
        "\n",
        "                for obj_id, box in zip(ids, xyxy):\n",
        "                    key = f\"id{int(obj_id)}\"\n",
        "                    if key not in selected_ids:\n",
        "                        continue\n",
        "\n",
        "                    p_idx = selected_ids.index(key)\n",
        "                    color = colors[p_idx]\n",
        "\n",
        "                    x1, y1, x2, y2 = box.astype(float)\n",
        "                    cx = int(round((x1 + x2) / 2.0))\n",
        "                    cy = int(round(y2))\n",
        "\n",
        "                    if draw_trail and trail is not None:\n",
        "                        cv2.circle(trail, (cx, cy), point_radius, color, -1)\n",
        "\n",
        "                    cv2.circle(img, (cx, cy), point_radius, color, -1)\n",
        "                    cv2.putText(img, f\"P{p_idx+1}\", (cx + point_radius + 2, max(0, cy - point_radius - 2)),\n",
        "                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
        "\n",
        "        except Exception as e:\n",
        "            cv2.putText(img, f\"draw error: {e}\", (10, 30),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
        "\n",
        "        # optional ball point rendering\n",
        "        if ball_detections is not None:\n",
        "            dets = ball_detections[idx]\n",
        "            boxes = _extract_boxes(dets)\n",
        "            for b in boxes:\n",
        "                try:\n",
        "                    x1, y1, x2, y2 = map(float, b[:4])\n",
        "                except Exception:\n",
        "                    continue\n",
        "                cx = int(round((x1 + x2) / 2.0))\n",
        "                cy = int(round(y2))\n",
        "                cv2.circle(img, (cx, cy), ball_radius, ball_color, -1)\n",
        "                cv2.putText(img, \"Ball\", (cx + ball_radius + 2, max(0, cy)),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, ball_color, 2)\n",
        "\n",
        "        # composite trail on top of frame\n",
        "        if draw_trail and trail is not None:\n",
        "            img = cv2.addWeighted(img, float(trail_mix), trail, 1.0 - float(trail_mix), 0.0)\n",
        "\n",
        "        # optional ID legend\n",
        "        for i, sid in enumerate(selected_ids):\n",
        "            cv2.putText(img, f\"P{i+1}: {sid}\", (10, 20 + i * 20),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, colors[i], 2)\n",
        "\n",
        "        out.write(img)\n",
        "\n",
        "    out.release()\n",
        "    print(f\"Video saved at: {output_path}\")\n",
        "\n",
        "\n",
        "# =========================\n",
        "# RESULTS FOLDER (per video)\n",
        "# =========================\n",
        "\n",
        "# Use the same variable from your notebook that points to the input video.\n",
        "# In your case, you already use source in other cells.\n",
        "VIDEO_PATH = source  # ex: \"/content/drive/MyDrive/tennis_analysis/input_videos/Djokovic2.mp4\"\n",
        "\n",
        "video_name = os.path.splitext(os.path.basename(VIDEO_PATH))[0]\n",
        "\n",
        "RESULTS_BASE_DIR = \"/content/drive/MyDrive/tennis_analysis/results\"\n",
        "RESULTS_VIDEO_DIR = os.path.join(RESULTS_BASE_DIR, video_name)\n",
        "os.makedirs(RESULTS_VIDEO_DIR, exist_ok=True)\n",
        "\n",
        "# saída do vídeo dentro da pasta do vídeo\n",
        "OUTPUT_VIDEO_PATH = os.path.join(RESULTS_VIDEO_DIR, f\"{video_name}_tracked.mp4\")\n",
        "\n",
        "print(\"Results directory:\", RESULTS_VIDEO_DIR)\n",
        "print(\"Output video path:\", OUTPUT_VIDEO_PATH)\n",
        "\n",
        "# =========================\n",
        "# RUN (selection + render)\n",
        "# =========================\n",
        "\n",
        "players_detections_selected, original_ids, id_map = select_two_players_balanced(\n",
        "    player_detections, court_keypoints, alpha=0.5\n",
        ")\n",
        "\n",
        "draw_selected_players_and_court(\n",
        "    video_frames, results, original_ids, court_keypoints,\n",
        "    output_path=OUTPUT_VIDEO_PATH,\n",
        "    fps=30,\n",
        "    draw_trail=True,\n",
        "    trail_decay=0.96,\n",
        "    trail_mix=0.75,\n",
        "    point_radius=6\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMTEp3QFlAqS"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BASE PIPELINE — Homography, interpolation, and temporal smoothing\n",
        "#\n",
        "# This pipeline implements the minimum workflow required to convert\n",
        "# pixel coordinates into real-world coordinates (meters) using a homography,\n",
        "# followed by gap filling and temporal smoothing.\n",
        "#\n",
        "# Stages:\n",
        "# 1. Homography: maps player positions from the image plane (pixels) to the\n",
        "#    real court plane (meters) using detected court keypoints.\n",
        "#\n",
        "# 2. Linear interpolation: fills missing samples (NaNs) caused by short-lived\n",
        "#    detection/tracking dropouts.\n",
        "#\n",
        "# 3. Butterworth low-pass filter: reduces high-frequency jitter introduced by\n",
        "#    the detector and tracker, while preserving the main kinematic structure\n",
        "#    of the movement.\n",
        "#\n",
        "# 4. Metric computation: total distance, mean/max velocity, and acceleration.\n",
        "#\n",
        "# This pipeline is typically sufficient when tracking is stable and does not\n",
        "# contain significant outliers.\n",
        "# ============================================================\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import butter, filtfilt\n",
        "\n",
        "# --- REAL COURT REFERENCE POINTS (meters) ---\n",
        "real_coords = np.array([\n",
        "    [0, 0],\n",
        "    [0, 10.97],\n",
        "    [23.77, 0],\n",
        "    [23.77, 10.97],\n",
        "    [0, 1.37],\n",
        "    [23.77, 1.37],\n",
        "    [0, 9.6],\n",
        "    [23.77, 9.6],\n",
        "    [5.485, 1.37],\n",
        "    [5.485, 9.6],\n",
        "    [18.285, 1.37],\n",
        "    [18.285, 9.6],\n",
        "    [5.485, 5.485],\n",
        "    [18.285, 5.485]\n",
        "], dtype=np.float32)\n",
        "\n",
        "\n",
        "# --- HOMOGRAPHY + REAL-WORLD COORDINATES ---\n",
        "def compute_fixed_homography_and_real_coords(players_detections_selected, court_keypoints, real_coords):\n",
        "    real_coords = np.array(real_coords, dtype=np.float32)\n",
        "    img_pts = np.array(court_keypoints, dtype=np.float32).reshape(-1, 2)\n",
        "\n",
        "    H, status = cv2.findHomography(img_pts, real_coords)\n",
        "    if H is None or status is None or not status.all():\n",
        "        raise RuntimeError(\"Failed to compute homography using the first-frame keypoints.\")\n",
        "\n",
        "    n_frames = len(players_detections_selected)\n",
        "    real_coords_list = []\n",
        "\n",
        "    for i in range(n_frames):\n",
        "        px_coords = players_detections_selected.iloc[i].values.reshape(2, 2)  # [[x1,y1], [x2,y2]]\n",
        "        pts_homog = np.hstack([px_coords, np.ones((2, 1))])  # (2,3)\n",
        "\n",
        "        real_pts_homog = (H @ pts_homog.T).T  # (2,3)\n",
        "        real_pts = real_pts_homog[:, :2] / real_pts_homog[:, 2, np.newaxis]  # normalize\n",
        "\n",
        "        real_coords_list.append(real_pts.flatten().tolist())\n",
        "\n",
        "    players_real_coords = pd.DataFrame(\n",
        "        real_coords_list,\n",
        "        columns=[\"player1_x\", \"player1_y\", \"player2_x\", \"player2_y\"]\n",
        "    )\n",
        "    return players_real_coords\n",
        "\n",
        "\n",
        "# --- NaN INTERPOLATION (GAP FILLING) ---\n",
        "def interpolate_nan_coords(df):\n",
        "    df_interp = df.copy()\n",
        "    for col in df_interp.columns:\n",
        "        df_interp[col] = df_interp[col].interpolate(method=\"linear\", limit_direction=\"both\")\n",
        "        # use ffill()/bfill() to avoid FutureWarning from fillna(method=...)\n",
        "        df_interp[col] = df_interp[col].ffill().bfill()\n",
        "    return df_interp\n",
        "\n",
        "\n",
        "# --- BUTTERWORTH LOW-PASS FILTER ---\n",
        "def butterworth_filter(data, cutoff=0.3, fs=30, order=4):\n",
        "    nyq = 0.5 * fs\n",
        "    normal_cutoff = cutoff / nyq\n",
        "    b, a = butter(order, normal_cutoff, btype=\"low\", analog=False)\n",
        "\n",
        "    filtered_data = []\n",
        "    n_players = len(data[0])\n",
        "\n",
        "    # process each player independently\n",
        "    for pid in range(n_players):\n",
        "        x = np.array([frame[pid][0] for frame in data])\n",
        "        y = np.array([frame[pid][1] for frame in data])\n",
        "        x_filt = filtfilt(b, a, x)\n",
        "        y_filt = filtfilt(b, a, y)\n",
        "        filtered_data.append(np.stack([x_filt, y_filt], axis=1))\n",
        "\n",
        "    # reconstruct per-frame structure\n",
        "    filtered_by_frame = []\n",
        "    for i in range(len(data)):\n",
        "        frame_coords = [filtered_data[pid][i] for pid in range(n_players)]\n",
        "        filtered_by_frame.append(frame_coords)\n",
        "\n",
        "    return filtered_by_frame\n",
        "\n",
        "\n",
        "# --- METRIC COMPUTATION ---\n",
        "def compute_metrics(coords_per_frame, fs=30):\n",
        "    n_players = len(coords_per_frame[0])\n",
        "    distances = {pid: 0.0 for pid in range(n_players)}\n",
        "    max_vel = {pid: 0.0 for pid in range(n_players)}\n",
        "    mean_vel = {pid: 0.0 for pid in range(n_players)}\n",
        "    max_accel = {pid: 0.0 for pid in range(n_players)}\n",
        "    max_decel = {pid: 0.0 for pid in range(n_players)}\n",
        "\n",
        "    velocities = {pid: [] for pid in range(n_players)}\n",
        "\n",
        "    # velocities\n",
        "    for i in range(1, len(coords_per_frame)):\n",
        "        dt = 1.0 / fs\n",
        "        for pid in range(n_players):\n",
        "            prev = np.array(coords_per_frame[i - 1][pid])\n",
        "            curr = np.array(coords_per_frame[i][pid])\n",
        "            dist = np.linalg.norm(curr - prev)\n",
        "            vel = dist / dt  # m/s\n",
        "            distances[pid] += dist\n",
        "            velocities[pid].append(vel)\n",
        "            if vel > max_vel[pid]:\n",
        "                max_vel[pid] = vel\n",
        "\n",
        "    # accelerations\n",
        "    for pid in range(n_players):\n",
        "        v = np.array(velocities[pid])\n",
        "        if len(v) > 1:\n",
        "            a = np.diff(v) * fs  # m/s²\n",
        "            max_accel[pid] = np.max(a)\n",
        "            max_decel[pid] = np.min(a)\n",
        "            mean_vel[pid] = np.mean(v)\n",
        "\n",
        "    return distances, max_vel, mean_vel, max_accel, max_decel\n",
        "\n",
        "\n",
        "# --- DATAFRAME -> LIST[FRAME] ---\n",
        "def df_to_frame_list(df):\n",
        "    frame_list = []\n",
        "    for _, row in df.iterrows():\n",
        "        coords = [(row[\"player1_x\"], row[\"player1_y\"]), (row[\"player2_x\"], row[\"player2_y\"])]\n",
        "        frame_list.append(coords)\n",
        "    return frame_list\n",
        "\n",
        "\n",
        "# --- TRAJECTORY PLOTTING ---\n",
        "def plot_trajectory(player_coords_filt, save_path=None, show=True, dpi=300):\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "    # outer court (doubles)\n",
        "    ax.plot([0, 23.77], [0, 0], \"k\")\n",
        "    ax.plot([23.77, 23.77], [0, 10.97], \"k\")\n",
        "    ax.plot([0, 23.77], [10.97, 10.97], \"k\")\n",
        "    ax.plot([0, 0], [0, 10.97], \"k\")\n",
        "    ax.plot([23.77 / 2, 23.77 / 2], [0, 10.97], \"k\", linewidth=3)\n",
        "\n",
        "    # service boxes\n",
        "    ax.plot([0, 23.77], [1.37, 1.37], \"k\")\n",
        "    ax.plot([23.77, 23.77], [1.37, 9.6], \"k\")\n",
        "    ax.plot([0, 23.77], [9.6, 9.6], \"k\")\n",
        "    ax.plot([0, 0], [0, 9.6], \"k\")\n",
        "\n",
        "    # vertical service lines\n",
        "    ax.plot([5.485, 5.485], [1.37, 9.6], \"k\")\n",
        "    ax.plot([18.285, 18.285], [1.37, 9.6], \"k\")\n",
        "\n",
        "    # middle service line (net-side)\n",
        "    ax.plot([5.485, 23.77 / 2], [10.97 / 2, 10.97 / 2], \"k\")\n",
        "    ax.plot([23.77 / 2, 18.285], [10.97 / 2, 10.97 / 2], \"k\")\n",
        "\n",
        "    ax.set_xlim([-8, 23.77 + 8])\n",
        "    ax.set_ylim([-2, 10.97 + 2])\n",
        "    ax.set_aspect(\"equal\")\n",
        "    ax.set_xlabel(\"X (m)\")\n",
        "    ax.set_ylabel(\"Y (m)\")\n",
        "\n",
        "    # trajectories\n",
        "    n_players = len(player_coords_filt[0])\n",
        "    for pid in range(n_players):\n",
        "        traj = np.array([frame[pid] for frame in player_coords_filt], dtype=float)\n",
        "        ax.plot(traj[:, 0], traj[:, 1], marker=\"o\", label=f\"Player {pid + 1}\")\n",
        "\n",
        "    ax.legend()\n",
        "\n",
        "    # ✅ salva a figura certa (fig), antes do show\n",
        "    if save_path is not None:\n",
        "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "        fig.savefig(save_path, dpi=dpi, bbox_inches=\"tight\")\n",
        "        print(\"Saved trajectory plot:\", save_path)\n",
        "\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close(fig)\n",
        "\n",
        "    return fig, ax\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# RESULTS DIR (per video) — ensure it exists here too\n",
        "# ============================================================\n",
        "VIDEO_PATH = source  # seu path do vídeo de entrada\n",
        "video_name = os.path.splitext(os.path.basename(VIDEO_PATH))[0]\n",
        "\n",
        "RESULTS_BASE_DIR = \"/content/drive/MyDrive/tennis_analysis/results\"\n",
        "RESULTS_VIDEO_DIR = os.path.join(RESULTS_BASE_DIR, video_name)\n",
        "os.makedirs(RESULTS_VIDEO_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Saving outputs to:\", RESULTS_VIDEO_DIR)\n",
        "\n",
        "# ============================================================\n",
        "# PIPELINE\n",
        "# ============================================================\n",
        "\n",
        "# 1) Real-world coordinates via homography\n",
        "players_real_coords = compute_fixed_homography_and_real_coords(\n",
        "    players_detections_selected,\n",
        "    court_keypoints,\n",
        "    real_coords\n",
        ")\n",
        "\n",
        "# (opcional, mas útil) salvar coordenadas\n",
        "players_real_coords.to_csv(os.path.join(RESULTS_VIDEO_DIR, \"real_coordinates.csv\"), index=False)\n",
        "\n",
        "# 2) Fill gaps (NaNs)\n",
        "players_real_coords_interp = interpolate_nan_coords(players_real_coords)\n",
        "\n",
        "# (opcional) salvar coordenadas interpoladas\n",
        "players_real_coords_interp.to_csv(os.path.join(RESULTS_VIDEO_DIR, \"filtered_coordinates.csv\"), index=False)\n",
        "\n",
        "# 3) Convert to per-frame list\n",
        "player_coords_list = df_to_frame_list(players_real_coords_interp)\n",
        "\n",
        "# 4) Temporal smoothing (Butterworth)\n",
        "player_coords_filt = butterworth_filter(player_coords_list, cutoff=1, fs=30, order=4)\n",
        "\n",
        "# 5) Metrics\n",
        "fs = 30\n",
        "distances, max_vel, mean_vel, max_accel, max_decel = compute_metrics(player_coords_filt, fs=fs)\n",
        "\n",
        "# 6) Print results\n",
        "for pid in distances.keys():\n",
        "    print(f\"\\n=== Player {pid+1} ===\")\n",
        "    print(f\"Total distance: {distances[pid]:.2f} m\")\n",
        "    print(f\"Mean velocity: {mean_vel[pid]:.2f} m/s\")\n",
        "    print(f\"Max velocity: {max_vel[pid]:.2f} m/s\")\n",
        "    print(f\"Max acceleration: {max_accel[pid]:.2f} m/s²\")\n",
        "    print(f\"Max deceleration: {max_decel[pid]:.2f} m/s²\")\n",
        "\n",
        "# ============================================================\n",
        "# SAVE METRICS CSV\n",
        "# ============================================================\n",
        "metrics_rows = []\n",
        "for pid in sorted(distances.keys()):\n",
        "    metrics_rows.append({\n",
        "        \"video\": os.path.basename(VIDEO_PATH),\n",
        "        \"player\": f\"player{pid+1}\",\n",
        "        \"total_distance_m\": float(distances[pid]),\n",
        "        \"mean_velocity_m_s\": float(mean_vel[pid]),\n",
        "        \"max_velocity_m_s\": float(max_vel[pid]),\n",
        "        \"max_acceleration_m_s2\": float(max_accel[pid]),\n",
        "        \"max_deceleration_m_s2\": float(max_decel[pid]),\n",
        "        \"fps\": fs\n",
        "    })\n",
        "\n",
        "metrics_df = pd.DataFrame(metrics_rows)\n",
        "metrics_path = os.path.join(RESULTS_VIDEO_DIR, \"metrics.csv\")\n",
        "metrics_df.to_csv(metrics_path, index=False, encoding=\"utf-8\")\n",
        "print(\"Saved metrics:\", metrics_path)\n",
        "\n",
        "# ============================================================\n",
        "# SAVE TRAJECTORY PLOT\n",
        "# ============================================================\n",
        "\n",
        "# Se sua plot_trajectory NÃO tem save_path, use este fallback:\n",
        "plot_path = os.path.join(RESULTS_VIDEO_DIR, \"trajectory.png\")\n",
        "\n",
        "# Opção A (recomendada): se você atualizou plot_trajectory(save_path=...)\n",
        "try:\n",
        "    plot_trajectory(player_coords_filt, save_path=plot_path)\n",
        "except TypeError:\n",
        "    # Opção B (fallback): salva a figura \"atual\" do matplotlib\n",
        "    plot_trajectory(player_coords_filt)\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.savefig(plot_path, dpi=300, bbox_inches=\"tight\")\n",
        "    print(\"Saved trajectory plot (fallback):\", plot_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
